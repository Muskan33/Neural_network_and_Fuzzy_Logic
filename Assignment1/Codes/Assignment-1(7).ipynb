{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment-1(7).ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"IYLKPOy8mBZb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuNZ9foax_y8","executionInfo":{"status":"ok","timestamp":1634063164449,"user_tz":-330,"elapsed":10,"user":{"displayName":"MUSKAN KHANNA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14503783361471552212"}}},"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt \n","import math\n","import copy"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDZn-s-5yC1d"},"source":["path = '/content/drive/MyDrive/CSV/data_q6_q7.xlsx' \n","df = pd.read_excel(path)\n","insts = df.to_numpy()\n","m= len(insts[:,0]) #No of instinces\n","ones = np.ones((m,1))\n","insts = np.append(ones,insts,axis=1)\n","n = len(insts[0,:])-1 #no of features including f0 n=31\n","for i in range(1,n,1):\n","  insts[:,i] = (insts[:,i] - insts[:,i].mean())/insts[:,i].std() #Normalize the data\n","insts_mb = copy.deepcopy(insts)\n","insts_s = copy.deepcopy(insts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzXhUp3JyGf4"},"source":["# hypothesis for logistic regretion which is the prediction \n","def hypothesis(w,insts,m,n):\n","  z = np.dot(insts[:,0:n],w.T)\n","  h = 1/(1+np.exp(-z.astype(np.float32))) #working\n","  return h\n","\n","# cost function for logistic regretion\n","def cost(h,insts,m):\n","  c = 0\n","  for i in range(m):\n","    c_te = (-insts_tr[i,-1]*np.log(h[i]) - (1- insts_tr[i,-1])*np.log(1 - h[i]))/m\n","    if not(np.isnan(c_te)) and not(math.isinf(c_te)):\n","      c = c+ c_te\n","  return c\n","# update of weight values for logistic regretion\n","def update(w,alpha,insts,h,m,n):\n","  d = np.zeros(n)\n","  for i in range(n):\n","    d[i] = np.dot( (h[0:m]- insts[0:m,-1]),insts[0:m,i] )\n","  for i in range(n):\n","    w[i] = w[i] - alpha*d[i]\n","\n","  return w\n","def update_stotastic(w,alpha,insts,h,index,n):\n","  d = np.zeros(n)\n","  for i in range(n):\n","    d[i] = (h[index]- insts[index,-1])*insts[index,i] \n","  for i in range(n):\n","    w[i] = w[i] - alpha*d[i]\n","\n","  return w\n","\n","def performance(mat,m):\n","  p = []\n","  for i in range(3):\n","    ia = mat[i,i]/(mat[i,0]+mat[i,1]+mat[i,2])\n","    p.append(ia)\n","  \n","  p.append((mat[0,0]+mat[1,1]+mat[2,2])/m)\n","  return p\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fs0LyAeGySYw"},"source":["K = 5 # no of folds\n","overall_accuracy = 0\n","accuracy_class1 = 0\n","accuracy_class2 = 0\n","accuracy_class3 = 0\n","for k in range(K):\n","  insts_te = insts[int((0.8-0.2*k)*m):int(m*(1-0.2*k)),:]\n","  insts_tr = np.delete(insts,np.s_[int((0.8-0.2*k)*m):int((1-0.2*k)*m)], axis=0)\n","  opt_weights = [] # stores the weight values for all the models\n","  opt_cost = [] # stores the final optimal cost value at the end of each group\n","  for l in range(1,4):\n","    train = copy.deepcopy(insts_tr)\n","    train[:,-1] = (train[:,-1]==i).astype(int) # assignes the value 1 if the level is i\n","    m_tr = len(insts_tr[:,0])\n","    itr = 600\n","    alpha = np.linspace(0.001,0.01,100)\n","    b_min = 10000\n","    c = np.ones(len(alpha))\n","    w_opt = np.zeros(n)  #stores the optimal weight value\n","    min=0\n","    for j in range(len(alpha)):\n","      w = np.random.rand(n)\n","      j_list = np.ones(itr)\n","      for i in range(itr):\n","        h = hypothesis(w,train,m_tr,n)\n","        w = update(w,alpha[j],train,h,m_tr,n)\n","      h = hypothesis(w,train,m_tr,n)\n","      c[j] = cost(h,train,m_tr)\n","      if c[j] < b_min:\n","        min = j\n","        b_min = c[j]\n","        w_opt = w\n","    \n","\n","\n","\n"],"execution_count":null,"outputs":[]}]}